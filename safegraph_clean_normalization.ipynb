{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INITIALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import time\n",
    "\n",
    "from itertools import chain\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, explode\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "\n",
    "# suppress warnings for readability\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# turn off scientific notation in pandas objects\n",
    "pd.set_option('display.float_format', lambda x: '%.8f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spark session\n",
    "spark = SparkSession.builder.appName('SafeGraph').config(\"spark.driver.memory\", \"15g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in 2019 ACS 5-year population data\n",
    "census_acs = \"/Users/esrieves/Documents/school/Research/foot_traffic/data/census/safegraph_open_census_data_2019/data/cbg_b01.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parser function\n",
    "# input: JSON dictionary element\n",
    "# output: parsed JSON element ready for explosion\n",
    "def parser(element):\n",
    "    parsed = json.loads(element)\n",
    "\n",
    "    if parsed is not None:\n",
    "        return parsed\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pattern/panel files\n",
    "# pattern: for each MSA, gets the file path and then 'walks' through all files in the subdirectory, creating a dataframe of all files\n",
    "# panel: does the same for each MSA's panel data\n",
    "# get patterns files for each MSA\n",
    "def get_pattern_files(MSA):\n",
    "    file_path = \"/Users/esrieves/Documents/school/Research/foot_traffic/data/Inputs/%s_MSA\" %MSA\n",
    "    file_ext_patterns = \"core_poi-patterns.csv.gz\"\n",
    "    \n",
    "    all_pattern_files = [file\n",
    "                for path, subdir, files in os.walk(file_path)\n",
    "                for file in glob(os.path.join(path, file_ext_patterns))]\n",
    "    \n",
    "    return(all_pattern_files)\n",
    "\n",
    "def get_panel_files(MSA):\n",
    "    file_path = \"/Users/esrieves/Documents/school/Research/foot_traffic/data/Inputs/%s_MSA\" %MSA\n",
    "    file_ext_panel = \"home_panel_summary.csv\"\n",
    "\n",
    "    all_panel_files = [file\n",
    "                for path, subdir, files in os.walk(file_path)\n",
    "                for file in glob(os.path.join(path, file_ext_panel))]\n",
    "    return(all_panel_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean pattern files function\n",
    "# reads in all csv files as a spark DF (w header, inferred schema); drops null on important columns; replaces leading 0; edits date;\n",
    "# trims NAICS to 2 #s, and selects important columns\n",
    "def read_clean_patterns(all_csv_files):\n",
    "    df = spark.read.option('header', 'True') \\\n",
    "        .option('inferSchema','True') \\\n",
    "        .option('escape', \"\\\"\") \\\n",
    "        .csv(all_csv_files)\n",
    "    \n",
    "    df = df.dropna(subset=[\"visitor_home_cbgs\", \"poi_cbg\", \"placekey\"])\n",
    "    df = df.where(\"visitor_home_cbgs!='{}'\")\n",
    "\n",
    "    # Add leading zero to poi_cbg column (lost in csv format)\n",
    "    # Remove additional NAICS code digits > 2\n",
    "    # Date range starts the last day of the month at 11 pm.. switching to the first day of the full month\n",
    "    # e.g. 5/31 to 6/1\n",
    "    df = df.withColumn(\"dest_cbg\", sf.format_string(\"%012d\",\"poi_cbg\"))\\\n",
    "        .withColumn(\"naics\", df.naics_code.substr(1,2))\\\n",
    "        .withColumn('date_start',sf.date_format(sf.date_add('date_range_start',1),\"MM-yyyy\"))\\\n",
    "        .withColumnRenamed('region','state')\\\n",
    "        .select('placekey', 'naics', 'state', 'date_start', 'raw_visitor_counts', 'raw_visit_counts', 'poi_cbg', 'visitor_home_cbgs',\n",
    "               'bucketed_dwell_times')\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_workers(patterns_df):\n",
    "    # Implement parser to parse and explode bucketed dwell time column\n",
    "    jsonudf = udf(parser, MapType(StringType(), IntegerType()))\n",
    "    \n",
    "    # parse then explode dwell time column to get > 240 minutes of dwelling (assumption: these are workers)\n",
    "    dwell_parsed = patterns_df.withColumn(\"parsed_dwell_times\", jsonudf(\"bucketed_dwell_times\"))\n",
    "    worker_cbgs_exploded = dwell_parsed.select(\"placekey\",\"raw_visit_counts\", \"date_start\",\"state\",\"visitor_home_cbgs\", \"poi_cbg\", \"naics\",explode(\"parsed_dwell_times\"))\n",
    "    \n",
    "    # filter 'workers' (> 240 min at poi)\n",
    "    # determine percent of visits (not visitors, bc bucketed dwell times is based on visits not visitors) > 240 min to determine percent workers\n",
    "    worker_cbgs_exploded = worker_cbgs_exploded.filter(worker_cbgs_exploded.key == \">240\")\\\n",
    "        .withColumn(\"pct_worker\",worker_cbgs_exploded.value/worker_cbgs_exploded.raw_visit_counts)\n",
    "    \n",
    "    # new df for analysis -- mostly visitors\n",
    "    # created and commented out mostly workers in case it's of future interest\n",
    "    #mostly_workers = worker_cbgs_exploded.where(\"pct_worker >= 0.3\")\n",
    "    mostly_visitors = worker_cbgs_exploded.where(\"pct_worker < 0.3\")\\\n",
    "        .select('placekey', 'naics', 'state', 'date_start', 'poi_cbg', 'visitor_home_cbgs')\n",
    "    \n",
    "    return(mostly_visitors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## census process function\n",
    "# reads in census data, maps state fips to state name to get population\n",
    "def census_process(census_dir):\n",
    "    df = spark.read.option('header', 'True') \\\n",
    "        .option('inferSchema','True') \\\n",
    "        .option('escape', \"\\\"\") \\\n",
    "        .csv(census_dir)\n",
    "    \n",
    "    # select cbg and population columns, rename\n",
    "    df = df.select('census_block_group', 'B01001e1')\\\n",
    "        .withColumnRenamed('B01001e1','population')\n",
    "    \n",
    "    fips_state_map = {\n",
    "        '01': 'AL', '02': 'AK', '04': 'AZ', '05': 'AR', '06': 'CA',\n",
    "        '08': 'CO', '09': 'CT', '10': 'DE', '11': 'DC', '12': 'FL',\n",
    "        '13': 'GA', '15': 'HI', '16': 'ID', '17': 'IL', '18': 'IN',\n",
    "        '19': 'IA', '20': 'KS', '21': 'KY', '22': 'LA', '23': 'ME',\n",
    "        '24': 'MD', '25': 'MA', '26': 'MI', '27': 'MN', '28': 'MS',\n",
    "        '29': 'MO', '30': 'MT', '31': 'NE', '32': 'NV', '33': 'NH',\n",
    "        '34': 'NJ', '35': 'NM', '36': 'NY', '37': 'NC', '38': 'ND',\n",
    "        '39': 'OH', '40': 'OK', '41': 'OR', '42': 'PA', '44': 'RI',\n",
    "        '45': 'SC', '46': 'SD', '47': 'TN', '48': 'TX', '49': 'UT',\n",
    "        '50': 'VT', '51': 'VA', '53': 'WA', '54': 'WV', '55': 'WI',\n",
    "        '56': 'WY', '72': 'PR'\n",
    "    }\n",
    "    \n",
    "    mapping_expr = sf.create_map([sf.lit(x) for x in chain(*fips_state_map.items())])\n",
    "    \n",
    "    # replace leading zero to ensure all states match\n",
    "    df = df.withColumn(\"cbg\", sf.format_string(\"%012d\",\"census_block_group\"))\n",
    "    \n",
    "    # create state fips column to match cbg with state name\n",
    "    df = df.withColumn('state_fips',df.cbg.substr(1,2))\\\n",
    "        .withColumn(\"state\", mapping_expr[sf.col(\"state_fips\")])\\\n",
    "        .groupby('state').agg(sf.sum('population').alias('state_pop'))\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read home panel function\n",
    "# read home_panel_summary file and summarize by date and state to calculate number of devices residing (used in aggregation)\n",
    "def read_home_panel(all_csv_files_panel):\n",
    "    df = spark.read.option('header', 'True') \\\n",
    "        .option('inferSchema','True') \\\n",
    "        .option('escape', \"\\\"\") \\\n",
    "        .csv(all_csv_files_panel)\n",
    "\n",
    "\n",
    "    df = df.withColumn('month_2dig',sf.format_string(\"%02d\",\"month\"))\\\n",
    "        .withColumn('date_start',sf.concat_ws('-',sf.col('month_2dig'),sf.col('year')))\\\n",
    "        .select(\"*\",sf.upper(sf.col('region')).alias('state'))\\\n",
    "        .groupby(['state', 'date_start'])\\\n",
    "        .agg(sf.sum('number_devices_residing').alias('number_of_devices_residing'))\n",
    "    \n",
    "    return(df)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to normalize and aggregate pattern data using census and panel data inputs\n",
    "def normalize_and_aggregate(home_panel_df,census_df,patterns_df):\n",
    "    df = home_panel_df.join(census_df, on = \"state\")\n",
    "    # create state multiplier column\n",
    "    df = df.withColumn(\"state_multiplier\", df.state_pop/df.number_of_devices_residing)\n",
    "    df_joined = df.join(patterns_df, on = ['date_start','state'])\n",
    "    \n",
    "    # Implement parser to parse and explode visitor home cbgs\n",
    "    jsonudf = udf(parser, MapType(StringType(), IntegerType()))\n",
    "    \n",
    "    # parse then explode visitor home cbgs to get sender cbg\n",
    "    visitor_home_cbgs_parsed = df_joined.withColumn(\"parsed_visitor_home_cbgs\", jsonudf(\"visitor_home_cbgs\"))\n",
    "    visitor_home_cbgs_exploded = visitor_home_cbgs_parsed.select(\"placekey\", \"poi_cbg\", \"naics\",\n",
    "                                                             \"date_start\",\"state_multiplier\",\n",
    "                                                             explode(\"parsed_visitor_home_cbgs\"))\n",
    "    \n",
    "    # use state multiplier to normalize visitor values and clean data (drop a few columns)\n",
    "    agg_df = visitor_home_cbgs_exploded.withColumn(\"sender_cbg\", visitor_home_cbgs_exploded.key) \\\n",
    "        .withColumn('norm_visitors',(sf.round((visitor_home_cbgs_exploded.value*visitor_home_cbgs_exploded.state_multiplier),0)))\\\n",
    "        .drop(\"placekey\")\\\n",
    "        .drop(\"key\")\\\n",
    "        .drop(\"value\")\\\n",
    "        .drop(\"state_multiplier\")\\\n",
    "        .drop(\"dest_tract\")\n",
    "    \n",
    "    # Aggregate visitors based on sender and destination cbgs, NAICS code\n",
    "    visitor_flows = agg_df.groupby([\"sender_cbg\",\"date_start\",\"naics\"])\\\n",
    "        .agg(sf.sum(\"norm_visitors\").alias(\"monthly_visitors_per_naics_tracts_NORMALIZED\"))\n",
    "    \n",
    "    return(visitor_flows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished file accumulation, starting flows\n",
      "finished with flows\n",
      "+------------+----------+-----+--------------------------------------------+\n",
      "|  sender_cbg|date_start|naics|monthly_visitors_per_naics_tracts_NORMALIZED|\n",
      "+------------+----------+-----+--------------------------------------------+\n",
      "|100030166042|   02-2018|   61|                                        86.0|\n",
      "|240419605022|   06-2021|   72|                                      1587.0|\n",
      "|240135120001|   06-2021|   72|                                      3991.0|\n",
      "|245101402002|   06-2021|   61|                                       365.0|\n",
      "|240276012015|   06-2021|   61|                                      1914.0|\n",
      "|245102606056|   06-2021|   72|                                      3703.0|\n",
      "|240135010012|   06-2021|   71|                                      3786.0|\n",
      "|240054205001|   06-2021|   44|                                      9565.0|\n",
      "|100030139042|   06-2021|   62|                                        73.0|\n",
      "|245102709033|   06-2021|   71|                                       693.0|\n",
      "|245102505003|   06-2021|   72|                                      6383.0|\n",
      "|240037312024|   06-2021|   72|                                      6561.0|\n",
      "|240253051004|   06-2021|   62|                                      1058.0|\n",
      "|240119554002|   06-2021|   44|                                       365.0|\n",
      "|371950014002|   06-2021|   44|                                       146.0|\n",
      "|240054307002|   06-2021|   53|                                      6317.0|\n",
      "|245101501003|   06-2021|   53|                                      2387.0|\n",
      "|240037310032|   06-2021|   53|                                      5476.0|\n",
      "|240338005172|   06-2021|   53|                                      1966.0|\n",
      "|240217517013|   06-2021|   53|                                       438.0|\n",
      "+------------+----------+-----+--------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "finished writing file\n"
     ]
    }
   ],
   "source": [
    "census_data = census_process(census_acs)\n",
    "\n",
    "# read and clean patterns and panels\n",
    "pattern_files = get_pattern_files(\"Baltimore\")\n",
    "clean_patterns = read_clean_patterns(pattern_files)\n",
    "\n",
    "# filter out predominately 'worker' pois (30%+ visits are > 6 hour duration)\n",
    "visitor_patterns = filter_workers(clean_patterns)\n",
    "\n",
    "panel_files = get_panel_files(\"Baltimore\")\n",
    "clean_panels = read_home_panel(panel_files)\n",
    "    \n",
    "print(\"finished file accumulation, starting flows\")\n",
    "    \n",
    "output_data = normalize_and_aggregate(clean_panels,census_data,visitor_patterns)\n",
    "    \n",
    "print(\"finished with flows\")\n",
    "output_data.show()\n",
    "print(\"finished writing file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NYC\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing NYC\n",
      "finished writing file NYC\n",
      "164.46701407432556\n",
      "LA\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing LA\n",
      "finished writing file LA\n",
      "131.69102907180786\n",
      "Chicago\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing Chicago\n",
      "finished writing file Chicago\n",
      "97.09180283546448\n",
      "Dallas\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing Dallas\n",
      "finished writing file Dallas\n",
      "103.14730596542358\n",
      "Houston\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing Houston\n",
      "finished writing file Houston\n",
      "82.41764283180237\n",
      "DC\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing DC\n",
      "finished writing file DC\n",
      "65.92621207237244\n",
      "Philadelphia\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing Philadelphia\n",
      "finished writing file Philadelphia\n",
      "56.66780614852905\n",
      "Miami\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing Miami\n",
      "finished writing file Miami\n",
      "77.62936973571777\n",
      "Atlanta\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing Atlanta\n",
      "finished writing file Atlanta\n",
      "78.5537850856781\n",
      "Boston\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing Boston\n",
      "finished writing file Boston\n",
      "43.315792083740234\n",
      "Phoenix\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing Phoenix\n",
      "finished writing file Phoenix\n",
      "51.22294783592224\n",
      "SanFrancisco\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing SanFrancisco\n",
      "finished writing file SanFrancisco\n",
      "53.71795392036438\n",
      "Riverside\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing Riverside\n",
      "finished writing file Riverside\n",
      "42.5712308883667\n",
      "Detroit\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing Detroit\n",
      "finished writing file Detroit\n",
      "45.12625789642334\n",
      "Seattle\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing Seattle\n",
      "finished writing file Seattle\n",
      "40.466959714889526\n",
      "Minneapolis\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing Minneapolis\n",
      "finished writing file Minneapolis\n",
      "41.46481513977051\n",
      "SanDiego\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing SanDiego\n",
      "finished writing file SanDiego\n",
      "39.244112968444824\n",
      "Tampa\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing Tampa\n",
      "finished writing file Tampa\n",
      "44.69599008560181\n",
      "Denver\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing Denver\n",
      "finished writing file Denver\n",
      "37.11547303199768\n",
      "Baltimore\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing Baltimore\n",
      "finished writing file Baltimore\n",
      "33.08458685874939\n"
     ]
    }
   ],
   "source": [
    "msas = ['NYC','LA','Chicago','Dallas','Houston','DC','Philadelphia','Miami','Atlanta','Boston','Phoenix','SanFrancisco',\n",
    "       'Riverside','Detroit','Seattle','Minneapolis','SanDiego','Tampa','Denver','Baltimore']\n",
    "\n",
    "\n",
    "# only need to run census data once\n",
    "census_data = census_process(census_acs)\n",
    "\n",
    "# for loop to run functions on all msas in list\n",
    "for msa in msas:\n",
    "    # start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    msa_str = str(msa)\n",
    "    print(msa_str)\n",
    "    \n",
    "    # read and clean patterns data\n",
    "    pattern_files = get_pattern_files(msa_str)\n",
    "    clean_patterns = read_clean_patterns(pattern_files)\n",
    "        \n",
    "    # filter out predominately 'worker' pois (30%+ visits are > 6 hour duration)\n",
    "    visitor_patterns = filter_workers(clean_patterns)\n",
    "    \n",
    "    # read and clean panel data for normalization\n",
    "    panel_files = get_panel_files(msa_str)\n",
    "    clean_panels = read_home_panel(panel_files)\n",
    "    \n",
    "    print(\"finished file accumulation, starting flows\")\n",
    "    \n",
    "    # normalize data using state multiplier, aggregate to cbg/date/naics level\n",
    "    output_data = normalize_and_aggregate(clean_panels,census_data,visitor_patterns)\n",
    "    \n",
    "    print(\"finished with flows, now writing %s\" %msa_str)\n",
    "    \n",
    "    # write normalized data to a single csv file\n",
    "    output_data.coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .option(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\",\"false\")\\\n",
    "    .option(\"header\",\"True\")\\\n",
    "    .csv(\"/Users/esrieves/Documents/school/Research/foot_traffic/data/Outputs/normalized_output_data/%s_MSA_18to21_visitor_flows\" %msa_str)\n",
    "    print(\"finished writing file %s\" %msa_str)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
