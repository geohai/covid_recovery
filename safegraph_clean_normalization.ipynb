{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import time\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, explode\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "\n",
    "# suppress warnings for readability\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# turn off scientific notation in pandas objects\n",
    "pd.set_option('display.float_format', lambda x: '%.8f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spark session\n",
    "spark = SparkSession.builder.appName('SafeGraph').config(\"spark.driver.memory\", \"15g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in 2019 ACS 5-year population data and SafeGraph's home_panel\n",
    "census_dir = \"/Users/esrieves/Documents/school/Research/foot_traffic/data/census/safegraph_open_census_data_2019/data/cbg_b01.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read patterns data + clean\n",
    "def read_patterns_data(all_csv_files):\n",
    "    \n",
    "    dtypes = {'poi_cbg':str,'naics_code':str}\n",
    "    df_files = (pd.read_csv(f, compression=\"gzip\",converters=dtypes) for f in all_csv_files)\n",
    "    df = pd.concat(df_files, ignore_index=True)\n",
    "    df = df.dropna(subset=[\"visitor_home_cbgs\", \"poi_cbg\", \"placekey\"]) # drop empty visitor home cbgs\n",
    "    return(df)\n",
    "\n",
    "def trim_patterns_columns(patterns_df):\n",
    "    \n",
    "    keep_cols = [\n",
    "        'placekey', 'naics_code', 'region',  'date_range_start',\n",
    "        'date_range_end', 'raw_visitor_counts', 'poi_cbg', 'visitor_home_cbgs'\n",
    "    ]\n",
    "    \n",
    "    # trim excess after .\n",
    "    # convert naics to broader 2 digit\n",
    "    patterns_df = patterns_df[keep_cols].assign(\n",
    "            date_range_start = patterns_df['date_range_start'].str[:10],\n",
    "            date_range_end = patterns_df['date_range_end'].str[:10],\n",
    "            naics_code = patterns_df['naics_code'].str[:2],\n",
    "            poi_cbg = patterns_df['poi_cbg'].str[:12]\n",
    "        )\n",
    "    \n",
    "    return(patterns_df)\n",
    "\n",
    "def read_and_trim_patterns(patterns_dir):\n",
    "    \n",
    "    patterns = read_patterns_data(patterns_dir)\n",
    "    patterns = trim_patterns_columns(patterns)\n",
    "    \n",
    "    return(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_clean_patterns(all_csv_files):\n",
    "    df = spark.read.option('header', 'True') \\\n",
    "        .option('inferSchema','True') \\\n",
    "        .option('escape', \"\\\"\") \\\n",
    "        .csv(all_csv_files)\n",
    "    \n",
    "    df = df.dropna(subset=[\"visitor_home_cbgs\", \"poi_cbg\", \"placekey\"])\n",
    "    df = df.where(\"visitor_home_cbgs!='{}'\")\n",
    "\n",
    "    # Add leading zero to poi_cbg column (lost in csv format)\n",
    "    df = df.withColumn(\"dest_cbg\", sf.format_string(\"%012d\",\"poi_cbg\"))\n",
    "\n",
    "    # Remove additional NAICS code digits > 2\n",
    "    df = df.withColumn(\"naics\", df.naics_code.substr(1,2))\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def census_process(census_dir):\n",
    "    df = spark.read.option('header', 'True') \\\n",
    "        .option('inferSchema','True') \\\n",
    "        .option('escape', \"\\\"\") \\\n",
    "        .csv(census_dir)\n",
    "    \n",
    "    # select cbg and population columns, rename\n",
    "    df = df.select('census_block_group', 'B01001e1')\\\n",
    "        .withColumnRenamed('B01001e1','population')\n",
    "    \n",
    "    fips_state_map = {\n",
    "        '01': 'AL', '02': 'AK', '04': 'AZ', '05': 'AR', '06': 'CA',\n",
    "        '08': 'CO', '09': 'CT', '10': 'DE', '11': 'DC', '12': 'FL',\n",
    "        '13': 'GA', '15': 'HI', '16': 'ID', '17': 'IL', '18': 'IN',\n",
    "        '19': 'IA', '20': 'KS', '21': 'KY', '22': 'LA', '23': 'ME',\n",
    "        '24': 'MD', '25': 'MA', '26': 'MI', '27': 'MN', '28': 'MS',\n",
    "        '29': 'MO', '30': 'MT', '31': 'NE', '32': 'NV', '33': 'NH',\n",
    "        '34': 'NJ', '35': 'NM', '36': 'NY', '37': 'NC', '38': 'ND',\n",
    "        '39': 'OH', '40': 'OK', '41': 'OR', '42': 'PA', '44': 'RI',\n",
    "        '45': 'SC', '46': 'SD', '47': 'TN', '48': 'TX', '49': 'UT',\n",
    "        '50': 'VT', '51': 'VA', '53': 'WA', '54': 'WV', '55': 'WI',\n",
    "        '56': 'WY', '72': 'PR'\n",
    "    }\n",
    "    \n",
    "    # functions for spark imported as sf\n",
    "    mapping_fips = sf.create_map([sf.lit(x) in chain(*map_dict.items())])\n",
    "    \n",
    "    \n",
    "    state_fips = mapping_fips(df.census_block_group.substr(1,2))\n",
    "    \n",
    "    # work on this to spark\n",
    "    state_pop = (\n",
    "        census_df\n",
    "        .assign(region = state_fips)\n",
    "        .groupby('region', as_index = False)['population'].sum()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read and process census data\n",
    "\n",
    "# read 2019 census data and find population by state\n",
    "def read_census_pop(census_dir):\n",
    "    \n",
    "    dtypes = {'census_block_group':str, 'B01001e1':int}\n",
    "    census_pop = pd.read_csv(census_dir, dtype = dtypes)[['census_block_group', 'B01001e1']].rename(columns = {'B01001e1':'population'})\n",
    "    \n",
    "    return(census_pop)\n",
    "\n",
    "def find_state_population(census_df):\n",
    "    \n",
    "    fips_state_map = {\n",
    "        '01': 'AL', '02': 'AK', '04': 'AZ', '05': 'AR', '06': 'CA',\n",
    "        '08': 'CO', '09': 'CT', '10': 'DE', '11': 'DC', '12': 'FL',\n",
    "        '13': 'GA', '15': 'HI', '16': 'ID', '17': 'IL', '18': 'IN',\n",
    "        '19': 'IA', '20': 'KS', '21': 'KY', '22': 'LA', '23': 'ME',\n",
    "        '24': 'MD', '25': 'MA', '26': 'MI', '27': 'MN', '28': 'MS',\n",
    "        '29': 'MO', '30': 'MT', '31': 'NE', '32': 'NV', '33': 'NH',\n",
    "        '34': 'NJ', '35': 'NM', '36': 'NY', '37': 'NC', '38': 'ND',\n",
    "        '39': 'OH', '40': 'OK', '41': 'OR', '42': 'PA', '44': 'RI',\n",
    "        '45': 'SC', '46': 'SD', '47': 'TN', '48': 'TX', '49': 'UT',\n",
    "        '50': 'VT', '51': 'VA', '53': 'WA', '54': 'WV', '55': 'WI',\n",
    "        '56': 'WY', '72': 'PR'\n",
    "    }\n",
    "    \n",
    "    state_fips = census_df['census_block_group'].str[:2].apply(lambda x: fips_state_map[x])\n",
    "    \n",
    "    state_pop = (\n",
    "        census_df\n",
    "        .assign(region = state_fips)\n",
    "        .groupby('region', as_index = False)['population'].sum()\n",
    "    )\n",
    "    \n",
    "    return(state_pop)\n",
    "\n",
    "def get_census_state_population(census_dir):\n",
    "    \n",
    "    census_state_population = read_census_pop(census_dir)\n",
    "    census_state_population = find_state_population(census_state_population)\n",
    "    \n",
    "    return(census_state_population)\n",
    "\n",
    "# read home_panel_summary file and summarize by date and state\n",
    "def read_home_panel(all_csv_files_panel, monthly = False):\n",
    "    dtypes = {'month':str, 'year':str, 'region':str, 'census_block_group':str, 'number_devices_residing':int}\n",
    "    \n",
    "    panel_files = (pd.read_csv(f, dtype=dtypes) for f in all_csv_files_panel)\n",
    "    home_panel = pd.concat(panel_files, ignore_index=True)\n",
    "    \n",
    "    if monthly:\n",
    "        home_panel = (\n",
    "            home_panel\n",
    "            .assign(date_range_start = pd.to_datetime(home_panel[['year', 'month']].assign(day = 1)))\n",
    "        )\n",
    "    \n",
    "    return(home_panel)\n",
    "    \n",
    "def group_home_panel(home_panel_df):\n",
    "    \n",
    "    home_panel_grouped = (\n",
    "        home_panel_df\n",
    "        .assign(\n",
    "            region = home_panel_df['region'].str.upper(),\n",
    "            date_range_start = home_panel_df['date_range_start'].astype(str).str[:10]\n",
    "        )\n",
    "        .groupby(['region', 'date_range_start'], as_index = False)['number_devices_residing'].sum()\n",
    "    )\n",
    "    \n",
    "    return(home_panel_grouped)\n",
    "\n",
    "def read_and_group_home_panel(home_panel_dir, monthly = False):\n",
    "    \n",
    "    home_panel = read_home_panel(home_panel_dir, monthly = monthly)\n",
    "    home_panel = group_home_panel(home_panel)\n",
    "    \n",
    "    return(home_panel)\n",
    "\n",
    "# join state census populations and group home_panel summary\n",
    "def join_census_home_panel(state_pop_df, state_home_panel_df):\n",
    "    \n",
    "    census_home_panel = state_home_panel_df.merge(state_pop_df, on = 'region')\n",
    "    census_home_panel = census_home_panel.assign(state_multiplier = census_home_panel['population'] / census_home_panel['number_devices_residing'])\n",
    "    \n",
    "    return(census_home_panel)\n",
    "\n",
    "# put it all together in a single function\n",
    "def read_and_join_census_home_panel(census_dir, home_panel_dir, monthly = False):\n",
    "    \n",
    "    state_pop_df = get_census_state_population(census_dir)\n",
    "    state_home_panel_df = read_and_group_home_panel(home_panel_dir, monthly = monthly)\n",
    "    census_home_panel = join_census_home_panel(state_pop_df, state_home_panel_df)\n",
    "    \n",
    "    return(census_home_panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parser function\n",
    "# input: JSON dictionary element\n",
    "# output: parsed JSON element ready for explosion\n",
    "def parser(element):\n",
    "    parsed = json.loads(element)\n",
    "\n",
    "    if parsed is not None:\n",
    "        return parsed\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## combine census data, patterns, and panel data to create state rate\n",
    "def census_patterns(pattern_files, census_file, home_panel_files):\n",
    "    # clean MSA data\n",
    "    patterns = read_and_trim_patterns(pattern_files)\n",
    "    \n",
    "    # read and join home panel and census data\n",
    "    census_home_panel_byState_monthly = read_and_join_census_home_panel(census_file, home_panel_files, monthly = True)\n",
    "    \n",
    "    # merge census/panel/patterns data \n",
    "    df_nrms = patterns.merge(census_home_panel_byState_monthly, on = ['date_range_start', 'region'], how = 'left')\n",
    "    \n",
    "    return(df_nrms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aggregate spark data to create visitor flows\n",
    "def spark_agg(spark_df):\n",
    "    # Aggregate visitors based on sender and destination tracts, NAICS code\n",
    "    visitor_flows = spark_df.groupby([\"sender_tract\",\"date_range_start\",\"naics_code\"])\\\n",
    "        .agg(sf.sum(\"norm_visitors\").alias(\"monthly_visitors_per_naics_tracts_NORMALIZED\"))\n",
    "    \n",
    "    return(visitor_flows)\n",
    "\n",
    "## Process w spark (json explode, use state multiplier, clean)\n",
    "def spark_process(df_nrms):\n",
    "    # reads pd df to spark df\n",
    "    df = spark.createDataFrame(df_nrms)\n",
    "    \n",
    "    \n",
    "    ## Implement parser to parse and explode visitor home cbgs\n",
    "    jsonudf = udf(parser, MapType(StringType(), IntegerType()))\n",
    "\n",
    "    visitor_home_cbgs_parsed = df.withColumn(\"parsed_visitor_home_cbgs\", jsonudf(\"visitor_home_cbgs\"))\n",
    "    visitor_home_cbgs_exploded = visitor_home_cbgs_parsed.select(\"placekey\", \"poi_cbg\", \"naics_code\",\n",
    "                                                             \"date_range_start\", \"date_range_end\",\"state_multiplier\",\n",
    "                                                             explode(\"parsed_visitor_home_cbgs\"))\n",
    "    \n",
    "    # use state multiplier and clean data, agg to tract level\n",
    "    df = visitor_home_cbgs_exploded.withColumn(\"dest_tract\", visitor_home_cbgs_exploded.poi_cbg.substr(1,11)) \\\n",
    "        .withColumn(\"sender_tract\", visitor_home_cbgs_exploded.key.substr(1,11)) \\\n",
    "        .withColumn('norm_visitors',(sf.round((visitor_home_cbgs_exploded.value*visitor_home_cbgs_exploded.state_multiplier),0)))\\\n",
    "        .drop(\"date_range_end\")\\\n",
    "        .drop(\"placekey\")\\\n",
    "        .drop(\"poi_cbg\")\\\n",
    "        .drop(\"key\")\\\n",
    "        .drop(\"value\")\\\n",
    "        .drop(\"state_multiplier\")\\\n",
    "        .drop(\"dest_tract\")\n",
    "    \n",
    "    visitor_flows = spark_agg(df)\n",
    "    \n",
    "    return(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get patterns files for each MSA\n",
    "def get_pattern_files(MSA):\n",
    "    file_path = \"/Users/esrieves/Documents/school/Research/foot_traffic/data/Inputs/%s_MSA\" %MSA\n",
    "    file_ext_patterns = \"core_poi-patterns.csv.gz\"\n",
    "    \n",
    "    all_pattern_files = [file\n",
    "                for path, subdir, files in os.walk(file_path)\n",
    "                for file in glob(os.path.join(path, file_ext_patterns))]\n",
    "    \n",
    "    return(all_pattern_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get panel files for each MSA\n",
    "def get_panel_files(MSA):\n",
    "    file_path = \"/Users/esrieves/Documents/school/Research/foot_traffic/data/Inputs/%s_MSA\" %MSA\n",
    "    file_ext_panel = \"home_panel_summary.csv\"\n",
    "\n",
    "    all_panel_files = [file\n",
    "                for path, subdir, files in os.walk(file_path)\n",
    "                for file in glob(os.path.join(path, file_ext_panel))]\n",
    "    return(all_panel_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN analyses of aggregating data, cleaning in spark\n",
    "def run_MSA(patterns,census,panel):\n",
    "    df = census_patterns(patterns,census,panel)\n",
    "    df_spark = spark_process(df)\n",
    "    return(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "pattern_files = get_pattern_files(\"NYC\")\n",
    "panel_files = get_panel_files(\"NYC\")\n",
    "flows = run_MSA(pattern_files,census_dir,panel_files)\n",
    "flows.write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .option(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\",\"false\")\\\n",
    "    .option(\"header\",\"True\")\\\n",
    "    .csv(\"/Users/esrieves/Documents/school/Research/foot_traffic/data/Outputs/test2/NYC_MSA_18to21_visitor_flows\")\n",
    "end_time = time.time()\n",
    "\n",
    "print(start_time - end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LA\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing LA\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "## RUN\n",
    "msas = ['LA','Chicago','Dallas','Houston','DC']\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for msa in msas:\n",
    "    msa_str = str(msa)\n",
    "    print(msa_str)\n",
    "    pattern_files = get_pattern_files(msa_str)\n",
    "    panel_files = get_panel_files(msa_str)\n",
    "    #output_string = \"/Users/esrieves/Documents/school/Research/foot_traffic/data/Outputs/top_20_normalized/%s_MSA_18to21_visitor_flows\" %msa_str\n",
    "    #print(output_string)\n",
    "    print(\"finished file accumulation, starting flows\")\n",
    "    \n",
    "    flows = run_MSA(pattern_files,census_dir,panel_files)\n",
    "    print(\"finished with flows, now writing %s\" %msa_str)\n",
    "    flows.write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .option(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\",\"false\")\\\n",
    "    .option(\"header\",\"True\")\\\n",
    "    .csv(\"/Users/esrieves/Documents/school/Research/foot_traffic/data/Outputs/test2/%s_MSA_18to21_visitor_flows\" %msa_str)\n",
    "    print(\"finished writing file %s\" %msa_str)\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "print(start_time - end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+------------+-------------+\n",
      "|naics_code|date_range_start|sender_tract|norm_visitors|\n",
      "+----------+----------------+------------+-------------+\n",
      "|        52|      2018-07-01| 36061011100|         45.0|\n",
      "|        52|      2018-07-01| 13121009801|         45.0|\n",
      "|        52|      2018-07-01| 13121008800|         45.0|\n",
      "|        52|      2018-07-01| 13121008902|         45.0|\n",
      "|        52|      2018-07-01| 13121010603|         45.0|\n",
      "|        81|      2018-07-01| 13015960700|         45.0|\n",
      "|        54|      2018-07-01| 13117130408|         45.0|\n",
      "|        54|      2018-07-01| 13121011504|         45.0|\n",
      "|        54|      2018-07-01| 13121011503|         79.0|\n",
      "|        54|      2018-07-01| 13121011503|         45.0|\n",
      "|        54|      2018-07-01| 13121011611|         45.0|\n",
      "|        54|      2018-07-01| 13117130302|         45.0|\n",
      "|        54|      2018-07-01| 13057090502|         79.0|\n",
      "|        54|      2018-07-01| 13121011619|         45.0|\n",
      "|        54|      2018-07-01| 13117130602|         91.0|\n",
      "|        54|      2018-07-01| 13117130506|         45.0|\n",
      "|        54|      2018-07-01| 13057090602|         45.0|\n",
      "|        61|      2018-07-01| 13135050313|         45.0|\n",
      "|        61|      2018-07-01| 13089021913|         45.0|\n",
      "|        61|      2018-07-01| 13135050311|         45.0|\n",
      "+----------+----------------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
