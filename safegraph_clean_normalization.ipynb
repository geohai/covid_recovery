{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import time\n",
    "\n",
    "from itertools import chain\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, explode\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "\n",
    "# suppress warnings for readability\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# turn off scientific notation in pandas objects\n",
    "pd.set_option('display.float_format', lambda x: '%.8f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spark session\n",
    "spark = SparkSession.builder.appName('SafeGraph').config(\"spark.driver.memory\", \"15g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in 2019 ACS 5-year population data and SafeGraph's home_panel\n",
    "census_acs = \"/Users/esrieves/Documents/school/Research/foot_traffic/data/census/safegraph_open_census_data_2019/data/cbg_b01.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTIONS -- SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parser function\n",
    "# input: JSON dictionary element\n",
    "# output: parsed JSON element ready for explosion\n",
    "def parser(element):\n",
    "    parsed = json.loads(element)\n",
    "\n",
    "    if parsed is not None:\n",
    "        return parsed\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get patterns files for each MSA\n",
    "def get_pattern_files(MSA):\n",
    "    file_path = \"/Users/esrieves/Documents/school/Research/foot_traffic/data/Inputs/%s_MSA\" %MSA\n",
    "    file_ext_patterns = \"core_poi-patterns.csv.gz\"\n",
    "    \n",
    "    all_pattern_files = [file\n",
    "                for path, subdir, files in os.walk(file_path)\n",
    "                for file in glob(os.path.join(path, file_ext_patterns))]\n",
    "    \n",
    "    return(all_pattern_files)\n",
    "\n",
    "def get_panel_files(MSA):\n",
    "    file_path = \"/Users/esrieves/Documents/school/Research/foot_traffic/data/Inputs/%s_MSA\" %MSA\n",
    "    file_ext_panel = \"home_panel_summary.csv\"\n",
    "\n",
    "    all_panel_files = [file\n",
    "                for path, subdir, files in os.walk(file_path)\n",
    "                for file in glob(os.path.join(path, file_ext_panel))]\n",
    "    return(all_panel_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_clean_patterns(all_csv_files):\n",
    "    df = spark.read.option('header', 'True') \\\n",
    "        .option('inferSchema','True') \\\n",
    "        .option('escape', \"\\\"\") \\\n",
    "        .csv(all_csv_files)\n",
    "    \n",
    "    df = df.dropna(subset=[\"visitor_home_cbgs\", \"poi_cbg\", \"placekey\"])\n",
    "    df = df.where(\"visitor_home_cbgs!='{}'\")\n",
    "\n",
    "    # Add leading zero to poi_cbg column (lost in csv format)\n",
    "    # Remove additional NAICS code digits > 2\n",
    "    # Date range starts the last day of the month at 11 pm.. switching to the first day of the full month\n",
    "    # e.g. 5/31 to 6/1\n",
    "    df = df.withColumn(\"dest_cbg\", sf.format_string(\"%012d\",\"poi_cbg\"))\\\n",
    "        .withColumn(\"naics\", df.naics_code.substr(1,2))\\\n",
    "        .withColumn('date_start',sf.date_format(sf.date_add('date_range_start',1),\"MM-yyyy\"))\\\n",
    "        .withColumnRenamed('region','state')\\\n",
    "        .select('placekey', 'naics', 'state', 'date_start', 'raw_visitor_counts', 'poi_cbg', 'visitor_home_cbgs')\n",
    " \n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def census_process(census_dir):\n",
    "    df = spark.read.option('header', 'True') \\\n",
    "        .option('inferSchema','True') \\\n",
    "        .option('escape', \"\\\"\") \\\n",
    "        .csv(census_dir)\n",
    "    \n",
    "    # select cbg and population columns, rename\n",
    "    df = df.select('census_block_group', 'B01001e1')\\\n",
    "        .withColumnRenamed('B01001e1','population')\n",
    "    \n",
    "    fips_state_map = {\n",
    "        '01': 'AL', '02': 'AK', '04': 'AZ', '05': 'AR', '06': 'CA',\n",
    "        '08': 'CO', '09': 'CT', '10': 'DE', '11': 'DC', '12': 'FL',\n",
    "        '13': 'GA', '15': 'HI', '16': 'ID', '17': 'IL', '18': 'IN',\n",
    "        '19': 'IA', '20': 'KS', '21': 'KY', '22': 'LA', '23': 'ME',\n",
    "        '24': 'MD', '25': 'MA', '26': 'MI', '27': 'MN', '28': 'MS',\n",
    "        '29': 'MO', '30': 'MT', '31': 'NE', '32': 'NV', '33': 'NH',\n",
    "        '34': 'NJ', '35': 'NM', '36': 'NY', '37': 'NC', '38': 'ND',\n",
    "        '39': 'OH', '40': 'OK', '41': 'OR', '42': 'PA', '44': 'RI',\n",
    "        '45': 'SC', '46': 'SD', '47': 'TN', '48': 'TX', '49': 'UT',\n",
    "        '50': 'VT', '51': 'VA', '53': 'WA', '54': 'WV', '55': 'WI',\n",
    "        '56': 'WY', '72': 'PR'\n",
    "    }\n",
    "    \n",
    "    mapping_expr = sf.create_map([sf.lit(x) for x in chain(*fips_state_map.items())])\n",
    "    \n",
    "    \n",
    "    df = df.withColumn('state_fips',df.census_block_group.substr(1,2))\\\n",
    "        .withColumn(\"state\", mapping_expr[sf.col(\"state_fips\")])\\\n",
    "        .groupby('state').agg(sf.sum('population').alias('state_pop'))\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read home_panel_summary file and summarize by date and state\n",
    "def read_home_panel(all_csv_files_panel):\n",
    "    df = spark.read.option('header', 'True') \\\n",
    "        .option('inferSchema','True') \\\n",
    "        .option('escape', \"\\\"\") \\\n",
    "        .csv(all_csv_files_panel)\n",
    "\n",
    "\n",
    "    df = df.withColumn('month_2dig',sf.format_string(\"%02d\",\"month\"))\\\n",
    "        .withColumn('date_start',sf.concat_ws('-',sf.col('month_2dig'),sf.col('year')))\\\n",
    "        .select(\"*\",sf.upper(sf.col('region')).alias('state'))\\\n",
    "        .groupby(['state', 'date_start'])\\\n",
    "        .agg(sf.sum('number_devices_residing').alias('number_of_devices_residing'))\n",
    "    \n",
    "    return(df)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_aggregate(home_panel_df,census_df,patterns_df):\n",
    "    df = home_panel_df.join(census_df, on = \"state\")\n",
    "    # create state multiplier column\n",
    "    df = df.withColumn(\"state_multiplier\", df.state_pop/df.number_of_devices_residing)\n",
    "    df_joined = df.join(patterns_df, on = ['date_start','state'])\n",
    "    \n",
    "    ## Implement parser to parse and explode visitor home cbgs\n",
    "    jsonudf = udf(parser, MapType(StringType(), IntegerType()))\n",
    "\n",
    "    visitor_home_cbgs_parsed = df_joined.withColumn(\"parsed_visitor_home_cbgs\", jsonudf(\"visitor_home_cbgs\"))\n",
    "    visitor_home_cbgs_exploded = visitor_home_cbgs_parsed.select(\"placekey\", \"poi_cbg\", \"naics\",\n",
    "                                                             \"date_start\",\"state_multiplier\",\n",
    "                                                             explode(\"parsed_visitor_home_cbgs\"))\n",
    "    \n",
    "    # use state multiplier and clean data, agg to tract level\n",
    "    agg_df = visitor_home_cbgs_exploded.withColumn(\"sender_cbg\", visitor_home_cbgs_exploded.key) \\\n",
    "        .withColumn('norm_visitors',(sf.round((visitor_home_cbgs_exploded.value*visitor_home_cbgs_exploded.state_multiplier),0)))\\\n",
    "        .drop(\"placekey\")\\\n",
    "        .drop(\"key\")\\\n",
    "        .drop(\"value\")\\\n",
    "        .drop(\"state_multiplier\")\\\n",
    "        .drop(\"dest_tract\")\n",
    "    \n",
    "    # Aggregate visitors based on sender and destination tracts, NAICS code\n",
    "    visitor_flows = agg_df.groupby([\"sender_cbg\",\"date_start\",\"naics\"])\\\n",
    "        .agg(sf.sum(\"norm_visitors\").alias(\"monthly_visitors_per_naics_tracts_NORMALIZED\"))\n",
    "    \n",
    "    return(visitor_flows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NYC\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing NYC\n",
      "finished writing file NYC\n",
      "204.69742679595947\n",
      "LA\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing LA\n",
      "finished writing file LA\n",
      "75.33873796463013\n",
      "Chicago\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing Chicago\n",
      "finished writing file Chicago\n",
      "97.61596608161926\n",
      "Dallas\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing Dallas\n",
      "finished writing file Dallas\n",
      "102.71193289756775\n",
      "Houston\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing Houston\n",
      "finished writing file Houston\n",
      "78.63655829429626\n",
      "DC\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing DC\n",
      "finished writing file DC\n",
      "59.974478006362915\n",
      "Philadelphia\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing Philadelphia\n",
      "finished writing file Philadelphia\n",
      "58.262129068374634\n",
      "Miami\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing Miami\n",
      "finished writing file Miami\n",
      "2960.5399689674377\n",
      "Atlanta\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing Atlanta\n",
      "finished writing file Atlanta\n",
      "1144.6633360385895\n",
      "Boston\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing Boston\n",
      "finished writing file Boston\n",
      "86.23857092857361\n",
      "Phoenix\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing Phoenix\n",
      "finished writing file Phoenix\n",
      "53.07595896720886\n",
      "SanFrancisco\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing SanFrancisco\n",
      "finished writing file SanFrancisco\n",
      "52.54116892814636\n",
      "Riverside\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing Riverside\n",
      "finished writing file Riverside\n",
      "46.290979862213135\n",
      "Detroit\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing Detroit\n",
      "finished writing file Detroit\n",
      "75.17390894889832\n",
      "Seattle\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing Seattle\n",
      "finished writing file Seattle\n",
      "72.28987288475037\n",
      "Minneapolis\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing Minneapolis\n",
      "finished writing file Minneapolis\n",
      "70.81695890426636\n",
      "SanDiego\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing SanDiego\n",
      "finished writing file SanDiego\n",
      "43.53718090057373\n",
      "Tampa\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing Tampa\n",
      "finished writing file Tampa\n",
      "3186.9572899341583\n",
      "Denver\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing Denver\n",
      "finished writing file Denver\n",
      "40.66917014122009\n",
      "Baltimore\n",
      "finished file accumulation, starting flows\n",
      "finished with flows, now writing Baltimore\n",
      "finished writing file Baltimore\n",
      "287.52139687538147\n"
     ]
    }
   ],
   "source": [
    "msas = ['NYC','LA','Chicago','Dallas','Houston','DC','Philadelphia','Miami','Atlanta','Boston','Phoenix','SanFrancisco',\n",
    "       'Riverside','Detroit','Seattle','Minneapolis','SanDiego','Tampa','Denver','Baltimore']\n",
    "\n",
    "# only need to run census data once\n",
    "census_data = census_process(census_acs)\n",
    "\n",
    "for msa in msas:\n",
    "    # start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    msa_str = str(msa)\n",
    "    print(msa_str)\n",
    "    \n",
    "    # read and clean patterns and panels\n",
    "    pattern_files = get_pattern_files(msa_str)\n",
    "    clean_patterns = read_clean_patterns(pattern_files)\n",
    "    \n",
    "    panel_files = get_panel_files(msa_str)\n",
    "    clean_panels = read_home_panel(panel_files)\n",
    "    \n",
    "    print(\"finished file accumulation, starting flows\")\n",
    "    \n",
    "    output_data = normalize_and_aggregate(clean_panels,census_data,clean_patterns)\n",
    "    \n",
    "    print(\"finished with flows, now writing %s\" %msa_str)\n",
    "    output_data.coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .option(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\",\"false\")\\\n",
    "    .option(\"header\",\"True\")\\\n",
    "    .csv(\"/Users/esrieves/Documents/school/Research/foot_traffic/data/Outputs/normalized_output_data3/%s_MSA_18to21_visitor_flows\" %msa_str)\n",
    "    print(\"finished writing file %s\" %msa_str)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "census = census_process(census_dir)\n",
    "#census.show()\n",
    "panel = read_home_panel(chi_panel)\n",
    "\n",
    "patterns = read_clean_patterns(chi_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-----+--------------------------------------------+\n",
      "|sender_tract|date_start|naics|monthly_visitors_per_naics_tracts_NORMALIZED|\n",
      "+------------+----------+-----+--------------------------------------------+\n",
      "| 17111870812|   07-2019|   72|                                     22862.0|\n",
      "| 17089853007|   07-2019|   62|                                      4313.0|\n",
      "| 17093890500|   07-2019|   62|                                     13571.0|\n",
      "| 13121011201|   07-2019|   72|                                        92.0|\n",
      "| 01089002200|   07-2019|   72|                                       160.0|\n",
      "| 17063000200|   07-2019|   72|                                     38019.0|\n",
      "| 13113140208|   07-2019|   44|                                       276.0|\n",
      "| 17037002000|   07-2019|   61|                                      2644.0|\n",
      "| 17031827801|   07-2019|   45|                                      9400.0|\n",
      "| 17031827700|   07-2019|   45|                                      4994.0|\n",
      "| 17031801300|   07-2019|   44|                                      9440.0|\n",
      "| 17031805902|   07-2019|   44|                                     22103.0|\n",
      "| 17031809700|   07-2019|   44|                                     11616.0|\n",
      "| 08031002901|   07-2019|   44|                                        46.0|\n",
      "| 17103000400|   07-2019|   44|                                       736.0|\n",
      "| 17089852906|   07-2019|   51|                                       996.0|\n",
      "| 17043842200|   07-2019|   72|                                     14754.0|\n",
      "| 17031820101|   07-2019|   72|                                     33119.0|\n",
      "| 17031220702|   07-2019|   72|                                     18627.0|\n",
      "| 17031806801|   07-2019|   71|                                     10329.0|\n",
      "+------------+----------+-----+--------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = normalize_and_aggregate(panel,census,patterns)\n",
    "t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTIONS -- PANDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read patterns data + clean\n",
    "def read_patterns_data(all_csv_files):\n",
    "    \n",
    "    dtypes = {'poi_cbg':str,'naics_code':str}\n",
    "    df_files = (pd.read_csv(f, compression=\"gzip\",converters=dtypes) for f in all_csv_files)\n",
    "    df = pd.concat(df_files, ignore_index=True)\n",
    "    df = df.dropna(subset=[\"visitor_home_cbgs\", \"poi_cbg\", \"placekey\"]) # drop empty visitor home cbgs\n",
    "    return(df)\n",
    "\n",
    "def trim_patterns_columns(patterns_df):\n",
    "    \n",
    "    keep_cols = [\n",
    "        'placekey', 'naics_code', 'region',  'date_range_start',\n",
    "        'date_range_end', 'raw_visitor_counts', 'poi_cbg', 'visitor_home_cbgs'\n",
    "    ]\n",
    "    \n",
    "    # trim excess after .\n",
    "    # convert naics to broader 2 digit\n",
    "    patterns_df = patterns_df[keep_cols].assign(\n",
    "            date_range_start = patterns_df['date_range_start'].str[:10],\n",
    "            date_range_end = patterns_df['date_range_end'].str[:10],\n",
    "            naics_code = patterns_df['naics_code'].str[:2],\n",
    "            poi_cbg = patterns_df['poi_cbg'].str[:12]\n",
    "        )\n",
    "    \n",
    "    return(patterns_df)\n",
    "\n",
    "def read_and_trim_patterns(patterns_dir):\n",
    "    \n",
    "    patterns = read_patterns_data(patterns_dir)\n",
    "    patterns = trim_patterns_columns(patterns)\n",
    "    \n",
    "    return(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read and process census data\n",
    "\n",
    "# read 2019 census data and find population by state\n",
    "def read_census_pop(census_dir):\n",
    "    \n",
    "    dtypes = {'census_block_group':str, 'B01001e1':int}\n",
    "    census_pop = pd.read_csv(census_dir, dtype = dtypes)[['census_block_group', 'B01001e1']].rename(columns = {'B01001e1':'population'})\n",
    "    \n",
    "    return(census_pop)\n",
    "\n",
    "def find_state_population(census_df):\n",
    "    \n",
    "    fips_state_map = {\n",
    "        '01': 'AL', '02': 'AK', '04': 'AZ', '05': 'AR', '06': 'CA',\n",
    "        '08': 'CO', '09': 'CT', '10': 'DE', '11': 'DC', '12': 'FL',\n",
    "        '13': 'GA', '15': 'HI', '16': 'ID', '17': 'IL', '18': 'IN',\n",
    "        '19': 'IA', '20': 'KS', '21': 'KY', '22': 'LA', '23': 'ME',\n",
    "        '24': 'MD', '25': 'MA', '26': 'MI', '27': 'MN', '28': 'MS',\n",
    "        '29': 'MO', '30': 'MT', '31': 'NE', '32': 'NV', '33': 'NH',\n",
    "        '34': 'NJ', '35': 'NM', '36': 'NY', '37': 'NC', '38': 'ND',\n",
    "        '39': 'OH', '40': 'OK', '41': 'OR', '42': 'PA', '44': 'RI',\n",
    "        '45': 'SC', '46': 'SD', '47': 'TN', '48': 'TX', '49': 'UT',\n",
    "        '50': 'VT', '51': 'VA', '53': 'WA', '54': 'WV', '55': 'WI',\n",
    "        '56': 'WY', '72': 'PR'\n",
    "    }\n",
    "    \n",
    "    state_fips = census_df['census_block_group'].str[:2].apply(lambda x: fips_state_map[x])\n",
    "    \n",
    "    state_pop = (\n",
    "        census_df\n",
    "        .assign(region = state_fips)\n",
    "        .groupby('region', as_index = False)['population'].sum()\n",
    "    )\n",
    "    \n",
    "    return(state_pop)\n",
    "\n",
    "def get_census_state_population(census_dir):\n",
    "    \n",
    "    census_state_population = read_census_pop(census_dir)\n",
    "    census_state_population = find_state_population(census_state_population)\n",
    "    \n",
    "    return(census_state_population)\n",
    "\n",
    "# read home_panel_summary file and summarize by date and state\n",
    "def read_home_panel(all_csv_files_panel, monthly = False):\n",
    "    dtypes = {'month':str, 'year':str, 'region':str, 'census_block_group':str, 'number_devices_residing':int}\n",
    "    \n",
    "    panel_files = (pd.read_csv(f, dtype=dtypes) for f in all_csv_files_panel)\n",
    "    home_panel = pd.concat(panel_files, ignore_index=True)\n",
    "    \n",
    "    if monthly:\n",
    "        home_panel = (\n",
    "            home_panel\n",
    "            .assign(date_range_start = pd.to_datetime(home_panel[['year', 'month']].assign(day = 1)))\n",
    "        )\n",
    "    \n",
    "    return(home_panel)\n",
    "    \n",
    "def group_home_panel(home_panel_df):\n",
    "    \n",
    "    home_panel_grouped = (\n",
    "        home_panel_df\n",
    "        .assign(\n",
    "            region = home_panel_df['region'].str.upper(),\n",
    "            date_range_start = home_panel_df['date_range_start'].astype(str).str[:10]\n",
    "        )\n",
    "        .groupby(['region', 'date_range_start'], as_index = False)['number_devices_residing'].sum()\n",
    "    )\n",
    "    \n",
    "    return(home_panel_grouped)\n",
    "\n",
    "def read_and_group_home_panel(home_panel_dir, monthly = False):\n",
    "    \n",
    "    home_panel = read_home_panel(home_panel_dir, monthly = monthly)\n",
    "    home_panel = group_home_panel(home_panel)\n",
    "    \n",
    "    return(home_panel)\n",
    "\n",
    "# join state census populations and group home_panel summary\n",
    "def join_census_home_panel(state_pop_df, state_home_panel_df):\n",
    "    \n",
    "    census_home_panel = state_home_panel_df.merge(state_pop_df, on = 'region')\n",
    "    census_home_panel = census_home_panel.assign(state_multiplier = census_home_panel['population'] / census_home_panel['number_devices_residing'])\n",
    "    \n",
    "    return(census_home_panel)\n",
    "\n",
    "# put it all together in a single function\n",
    "def read_and_join_census_home_panel(census_dir, home_panel_dir, monthly = False):\n",
    "    \n",
    "    state_pop_df = get_census_state_population(census_dir)\n",
    "    state_home_panel_df = read_and_group_home_panel(home_panel_dir, monthly = monthly)\n",
    "    census_home_panel = join_census_home_panel(state_pop_df, state_home_panel_df)\n",
    "    \n",
    "    return(census_home_panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parser function\n",
    "# input: JSON dictionary element\n",
    "# output: parsed JSON element ready for explosion\n",
    "def parser(element):\n",
    "    parsed = json.loads(element)\n",
    "\n",
    "    if parsed is not None:\n",
    "        return parsed\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## combine census data, patterns, and panel data to create state rate\n",
    "def census_patterns(pattern_files, census_file, home_panel_files):\n",
    "    # clean MSA data\n",
    "    patterns = read_and_trim_patterns(pattern_files)\n",
    "    \n",
    "    # read and join home panel and census data\n",
    "    census_home_panel_byState_monthly = read_and_join_census_home_panel(census_file, home_panel_files, monthly = True)\n",
    "    \n",
    "    # merge census/panel/patterns data \n",
    "    df_nrms = patterns.merge(census_home_panel_byState_monthly, on = ['date_range_start', 'region'], how = 'left')\n",
    "    \n",
    "    return(df_nrms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aggregate spark data to create visitor flows\n",
    "def spark_agg(spark_df):\n",
    "    # Aggregate visitors based on sender and destination tracts, NAICS code\n",
    "    visitor_flows = spark_df.groupby([\"sender_tract\",\"date_range_start\",\"naics_code\"])\\\n",
    "        .agg(sf.sum(\"norm_visitors\").alias(\"monthly_visitors_per_naics_tracts_NORMALIZED\"))\n",
    "    \n",
    "    return(visitor_flows)\n",
    "\n",
    "## Process w spark (json explode, use state multiplier, clean)\n",
    "def spark_process(df_nrms):\n",
    "    # reads pd df to spark df\n",
    "    df = spark.createDataFrame(df_nrms)\n",
    "    \n",
    "    \n",
    "    ## Implement parser to parse and explode visitor home cbgs\n",
    "    jsonudf = udf(parser, MapType(StringType(), IntegerType()))\n",
    "\n",
    "    visitor_home_cbgs_parsed = df.withColumn(\"parsed_visitor_home_cbgs\", jsonudf(\"visitor_home_cbgs\"))\n",
    "    visitor_home_cbgs_exploded = visitor_home_cbgs_parsed.select(\"placekey\", \"poi_cbg\", \"naics_code\",\n",
    "                                                             \"date_range_start\", \"date_range_end\",\"state_multiplier\",\n",
    "                                                             explode(\"parsed_visitor_home_cbgs\"))\n",
    "    \n",
    "    # use state multiplier and clean data, agg to tract level\n",
    "    df = visitor_home_cbgs_exploded.withColumn(\"dest_tract\", visitor_home_cbgs_exploded.poi_cbg.substr(1,11)) \\\n",
    "        .withColumn(\"sender_tract\", visitor_home_cbgs_exploded.key.substr(1,11)) \\\n",
    "        .withColumn('norm_visitors',(sf.round((visitor_home_cbgs_exploded.value*visitor_home_cbgs_exploded.state_multiplier),0)))\\\n",
    "        .drop(\"date_range_end\")\\\n",
    "        .drop(\"placekey\")\\\n",
    "        .drop(\"poi_cbg\")\\\n",
    "        .drop(\"key\")\\\n",
    "        .drop(\"value\")\\\n",
    "        .drop(\"state_multiplier\")\\\n",
    "        .drop(\"dest_tract\")\n",
    "    \n",
    "    visitor_flows = spark_agg(df)\n",
    "    \n",
    "    return(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get patterns files for each MSA\n",
    "def get_pattern_files(MSA):\n",
    "    file_path = \"/Users/esrieves/Documents/school/Research/foot_traffic/data/Inputs/%s_MSA\" %MSA\n",
    "    file_ext_patterns = \"core_poi-patterns.csv.gz\"\n",
    "    \n",
    "    all_pattern_files = [file\n",
    "                for path, subdir, files in os.walk(file_path)\n",
    "                for file in glob(os.path.join(path, file_ext_patterns))]\n",
    "    \n",
    "    return(all_pattern_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get panel files for each MSA\n",
    "def get_panel_files(MSA):\n",
    "    file_path = \"/Users/esrieves/Documents/school/Research/foot_traffic/data/Inputs/%s_MSA\" %MSA\n",
    "    file_ext_panel = \"home_panel_summary.csv\"\n",
    "\n",
    "    all_panel_files = [file\n",
    "                for path, subdir, files in os.walk(file_path)\n",
    "                for file in glob(os.path.join(path, file_ext_panel))]\n",
    "    return(all_panel_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN analyses of aggregating data, cleaning in spark\n",
    "def run_MSA(patterns,census,panel):\n",
    "    df = census_patterns(patterns,census,panel)\n",
    "    df_spark = spark_process(df)\n",
    "    return(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LA\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_pattern_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-ea03e6015255>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmsa_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsa_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mpattern_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pattern_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsa_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mpanel_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_panel_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsa_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#output_string = \"/Users/esrieves/Documents/school/Research/foot_traffic/data/Outputs/top_20_normalized/%s_MSA_18to21_visitor_flows\" %msa_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_pattern_files' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "## RUN\n",
    "\n",
    "chi = get_panel_files(\"Chicago\")\n",
    "msas = ['LA','Chicago','Dallas','Houston','DC']\n",
    "\n",
    "#start_time = time.time()\n",
    "\n",
    "for msa in msas:\n",
    "    start_time = time.time()\n",
    "    msa_str = str(msa)\n",
    "    print(msa_str)\n",
    "    pattern_files = get_pattern_files(msa_str)\n",
    "    panel_files = get_panel_files(msa_str)\n",
    "    #output_string = \"/Users/esrieves/Documents/school/Research/foot_traffic/data/Outputs/top_20_normalized/%s_MSA_18to21_visitor_flows\" %msa_str\n",
    "    #print(output_string)\n",
    "    print(\"finished file accumulation, starting flows\")\n",
    "    \n",
    "    flows = run_MSA(pattern_files,census_dir,panel_files)\n",
    "    print(\"finished with flows, now writing %s\" %msa_str)\n",
    "    flows.write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .option(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\",\"false\")\\\n",
    "    .option(\"header\",\"True\")\\\n",
    "    .csv(\"/Users/esrieves/Documents/school/Research/foot_traffic/data/Outputs/test2/%s_MSA_18to21_visitor_flows\" %msa_str)\n",
    "    print(\"finished writing file %s\" %msa_str)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(start_time - end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "pattern_files = get_pattern_files(\"NYC\")\n",
    "panel_files = get_panel_files(\"NYC\")\n",
    "flows = run_MSA(pattern_files,census_dir,panel_files)\n",
    "flows.write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .option(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\",\"false\")\\\n",
    "    .option(\"header\",\"True\")\\\n",
    "    .csv(\"/Users/esrieves/Documents/school/Research/foot_traffic/data/Outputs/test2/NYC_MSA_18to21_visitor_flows\")\n",
    "end_time = time.time()\n",
    "\n",
    "print(start_time - end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+------------+-------------+\n",
      "|naics_code|date_range_start|sender_tract|norm_visitors|\n",
      "+----------+----------------+------------+-------------+\n",
      "|        52|      2018-07-01| 36061011100|         45.0|\n",
      "|        52|      2018-07-01| 13121009801|         45.0|\n",
      "|        52|      2018-07-01| 13121008800|         45.0|\n",
      "|        52|      2018-07-01| 13121008902|         45.0|\n",
      "|        52|      2018-07-01| 13121010603|         45.0|\n",
      "|        81|      2018-07-01| 13015960700|         45.0|\n",
      "|        54|      2018-07-01| 13117130408|         45.0|\n",
      "|        54|      2018-07-01| 13121011504|         45.0|\n",
      "|        54|      2018-07-01| 13121011503|         79.0|\n",
      "|        54|      2018-07-01| 13121011503|         45.0|\n",
      "|        54|      2018-07-01| 13121011611|         45.0|\n",
      "|        54|      2018-07-01| 13117130302|         45.0|\n",
      "|        54|      2018-07-01| 13057090502|         79.0|\n",
      "|        54|      2018-07-01| 13121011619|         45.0|\n",
      "|        54|      2018-07-01| 13117130602|         91.0|\n",
      "|        54|      2018-07-01| 13117130506|         45.0|\n",
      "|        54|      2018-07-01| 13057090602|         45.0|\n",
      "|        61|      2018-07-01| 13135050313|         45.0|\n",
      "|        61|      2018-07-01| 13089021913|         45.0|\n",
      "|        61|      2018-07-01| 13135050311|         45.0|\n",
      "+----------+----------------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
